<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haresh Karnan</title>

  <meta name="author" content="Haresh Karnan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ut_seal.png">
  <style>
    :root {
      --bg-color: #ffffff;
      --text-color: #000000;
      --link-color: #60981e;
      --heading-color: #000000;
      --table-bg: #ffffff;
      --table-border: #dddddd;
    }

    [data-theme="dark"] {
      --bg-color: #1a1a1a;
      --text-color: #ffffff;
      --link-color: #90caf9;
      --heading-color: #ffffff;
      --table-bg: #2d2d2d;
      --table-border: #404040;
    }

    body {
      background-color: var(--bg-color);
      color: var(--text-color);
      transition: background-color 0.3s, color 0.3s;
    }

    a {
      color: var(--link-color);
    }

    heading {
      color: var(--heading-color);
    }

    table {
      background-color: var(--table-bg);
    }

    td {
      border-color: var(--table-border);
    }

    .theme-switch-wrapper {
      position: fixed;
      top: 20px;
      right: 20px;
      z-index: 1000;
    }

    .theme-switch {
      display: inline-block;
      height: 34px;
      position: relative;
      width: 60px;
    }

    .theme-switch input {
      display: none;
    }

    .slider {
      background-color: #ccc;
      bottom: 0;
      cursor: pointer;
      left: 0;
      position: absolute;
      right: 0;
      top: 0;
      transition: .4s;
      border-radius: 34px;
    }

    .slider:before {
      background-color: #fff;
      bottom: 4px;
      content: "";
      height: 26px;
      left: 4px;
      position: absolute;
      transition: .4s;
      width: 26px;
      border-radius: 50%;
    }

    input:checked + .slider {
      background-color: #60981e;
    }

    input:checked + .slider:before {
      transform: translateX(26px);
    }

    .slider:after {
      content: 'üåô';
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      font-size: 14px;
    }

    input:checked + .slider:after {
      content: '‚òÄÔ∏è';
    }

    .profile-container {
      position: relative;
      width: 100%;
      padding-bottom: 100%; /* Creates a square container */
      overflow: hidden;
      border-radius: 50%; /* Makes the container circular */
    }
    
    .profile-img, .profile-img-hover {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover; /* Ensures the image fills the circular container */
      transition: opacity 0.3s ease;
      border-radius: 50%; /* Makes the images circular */
    }
    
    .profile-img-hover {
      opacity: 0;
    }
    
    .profile-container:hover .profile-img {
      opacity: 0;
    }
    
    .profile-container:hover .profile-img-hover {
      opacity: 1;
    }
  </style>
</head>

<body>
  <div class="theme-switch-wrapper">
    <label class="theme-switch" for="checkbox">
      <input type="checkbox" id="checkbox" />
      <div class="slider"></div>
    </label>
  </div>

  <script>
    const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
    const currentTheme = localStorage.getItem('theme');

    if (currentTheme) {
      document.documentElement.setAttribute('data-theme', currentTheme);
      if (currentTheme === 'dark') {
        toggleSwitch.checked = true;
      }
    }

    function switchTheme(e) {
      if (e.target.checked) {
        document.documentElement.setAttribute('data-theme', 'dark');
        localStorage.setItem('theme', 'dark');
      } else {
        document.documentElement.setAttribute('data-theme', 'light');
        localStorage.setItem('theme', 'light');
      }    
    }

    toggleSwitch.addEventListener('change', switchTheme, false);
  </script>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haresh Karnan</name>
              </p>
              <p>Howdy! I'm a Machine Learning Researcher at <a href="https://zoox.com/">Zoox</a>, building foundation models for autonomy. I focus on large-scale training and validation of multimodal architectures that integrate perception, prediction, and planning to improve behavior understanding, response in complex scenarios, and validation for safer, more capable autonomous systems.
              </p>

              <p>
              I earned my PhD from The University of Texas at Austin, where I specialized in <strong>Reinforcement Learning</strong>, <strong>Self-Supervised Learning</strong> and <strong>Artificial Intelligence</strong> for Robotics. I was advised by <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>, and affiliated with the Learning Agents Research Group (LARG) at UT Austin Computer Science department.
              </p>

              <p>
                In my past life, I worked with <a href="https://bobskelton.github.io/">Dr. Robert Skelton</a> at Texas A&M University, College Station, on state estimation and control of Tensegrity robots.
              </p>

              <!-- <p align=center>
                <font color=#60981e><em>I'm on the job market for research positions in industry, starting early 2024.<br>Please feel free to reach out if you would like to chat!</em></font>
                </p> -->

              <p style="text-align:center">
                <strong>
                <a href="mailto:haresh.miriyala@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="data/HareshKarnan-bio.txt">Biography</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VatfufAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/hareshmiriyala">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/hareshkarnan/">LinkedIn</a>
                </strong>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/haresh_new.jpg">
                <div class="profile-container">
                  <img alt="profile photo" src="images/haresh_new_circle.png" class="profile-img">
                  <img alt="ghibli profile photo" src="images/haresh_ghibli.png" class="profile-img-hover">
                </div>
              </a>
            </td>
          </tr>
        </tbody></table>

        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- PhD Thesis -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/dissertation.png" alt="PhD Thesis" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://repositories.lib.utexas.edu/items/f9dec981-19a0-4e4c-9cac-7f63c7a68ad3" id="phd-thesis">
                <papertitle>Aligning Robot Navigation Behaviors with Human Intentions and Preferences</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>
              <br>
              <em>PhD Dissertation, The University of Texas at Austin</em>, 2024
              <br>
              <a href="https://repositories.lib.utexas.edu/items/f9dec981-19a0-4e4c-9cac-7f63c7a68ad3">Repository Link</a>
              <p>This dissertation addresses value misalignment in autonomous navigation by developing methods that ensure robot behaviors align with human intentions and preferences. It introduces novel techniques for imitation learning from demonstrations, self-supervised terrain representation learning for off-road navigation, and socially compliant navigation in human-occupied spaces, enabling robots to operate autonomously while respecting human values.</p>
            </td>
          </tr>

          <!-- PATERN -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/patern_night.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.09912" id="3DSP">
                <papertitle>Learning to Extrapolate Human Preferences for Preference-Aligned Path Planning</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Elvin Yang, Garrett Warnell, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.09912">Paper</a> / <a href="https://youtu.be/87dJWMk2X4c"> Video </a> / <a href="https://drive.google.com/file/d/1u7A3PQYSb74_5J46EV0FVuoKEDjg-uRJ/view?usp=sharing"> Poster </a> / <a href="https://youtu.be/j7159pE0u6s?si=cohnyhNMnH6h5oJx" > Robot Deployment </a>
              <p> Robots navigating off-road environments typically require additional human feedback to learn operator preferences for visually novel terrains. In this work, we introduce a framework to extrapolate operator preferences from known terrains by leveraging multi-modality. </p>
            </td>
          </tr>

          <!-- Social Case Study -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/social_case_study.png" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.13466" id="3DSP">
                <papertitle>Targeted Learning: A Hybrid Approach to Social Robot Navigation</papertitle>
              </a>
              <br>
              Amir Hossain Raj, Zichao Hu, <strong>Haresh Karnan</strong>, Rohan Chandra, Amirezza Payandeh, Luisa Mao, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://cs.gmu.edu/~xiao/">Xuesu Xiao</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2309.13466">Paper</a> / <a href="https://youtu.be/QT0JTuPhnqE?si=c7Vl62bILRrRMBAx"> Video </a>
              <p> In this work, we challenge the prevailing shift towards solely utilizing learning-based approaches for robot navigation in social contexts, advocating instead for a hybrid approach that toggles between classical and learning-based motion planners. </p>
            </td>
          </tr>

          
          <!-- STERLING -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/sterling.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.15302" id="3DSP">
                <papertitle>Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Elvin Yang, Daniel Farkash, Garrett Warnell, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2023
              <br>
              <a href="https://hareshkarnan.github.io/sterling/">Project Page</a> / <a href="https://arxiv.org/abs/2309.15302">Paper</a> / <a href="https://youtu.be/7WI41DfJQ2k"> Video </a> / <a href="https://youtu.be/dQb1XzocdtE" > Robot Deployment </a> / <a href="https://drive.google.com/file/d/1QHF8YUgIHwvK6pvGwa0zyNKLQKlCLLxA/view?usp=sharing"> Poster </a> / <a href="https://github.com/HareshKarnan/sterling_corl23"> Code </a>
              <p> Visual terrain awareness is crucial for autonomous off-road navigation. We propose a novel way to learn relevant terrain representations from unconstrained, multi-modal robot experience collected with any policy. We show how such learned representations enable aligning off-road navigation behaviors with human operator preferences.</p>
            </td>
          </tr>

          <!--          SCAND -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/scand.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.15041" id="3DSP">
                <papertitle>Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation</papertitle>
              </a>
              <strong>Haresh Karnan</strong>, Anirudh Nair, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Garrett Warnell, <a href="https://storage.googleapis.com/pirk.io/index.html">Soeren Pirk</a>, <a href="https://sites.google.com/view/alextoshev">Alexander Toshev</a>, <a href="http://justinhart.net/">Justin Hart</a>, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>IEEE Robotics and Automation Letters (IEEE RA-L) </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.15041">Paper</a> / <a href="https://www.youtube.com/watch?v=QqfgWmjOKkY" > Short Video </a> / <a href="https://youtu.be/QgBfMjWpQIw" > Long Video </a> / <a href="data/SCAND_poster.pdf"> Poster </a> / <a href="https://dataverse.tdl.org/dataset.xhtml?persistentId=doi:10.18738/T8/0PRYRH"> Dataset </a>
              <p> We introduce a large-scale, first-person-view dataset of socially compliant robot navigation demonstrations. <strong>SCAND</strong> consists of 138 trajectories, 25 miles of socially compliant navigation demonstrations collected on 2 robots by 4 human demonstrators within the UT Austin campus. </p>
            </td>
          </tr>

          <!--          VI-IKD -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/drive_fast.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.15983" id="3DSP">
                <papertitle>High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Kavan Singh Sikand, <a href="https://pranavatreya.github.io/">Pranav Atreya</a>, <a href="https://www.cs.utexas.edu/~srabiee/">Sadegh Rabiee</a>, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Garrett Warnell, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
               <br>
              <em> International Conference on Intelligent Robots and Systems (IROS) </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.15983">Paper</a> / <a href="https://youtu.be/tWaqwI9r8r0" > Video </a>
              <p> We introduce <strong>VI-IKD</strong>, a visual-inertial IKD model that learns to anticipate the kinodynamic interactions of the vehicle with the terrain ahead, enabling accurate high-speed navigation. </p>
            </td>
        </tr>

        <!--          Optim-FKD -->
        <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/truck_drift.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2206.08487" id="3DSP">
                <papertitle>Optim-FKD: High-Speed Accurate Robot Control using Learned Forward Kinodynamics and Non-linear Least Squares Optimization</papertitle>
              </a>
              <a href="https://pranavatreya.github.io/">Pranav Atreya</a>, <strong>Haresh Karnan</strong>, Kavan Singh Sikand, <a href="https://www.cs.utexas.edu/~srabiee/">Sadegh Rabiee</a>, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Garrett Warnell, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em> International Conference on Intelligent Robots and Systems (IROS) </em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2206.08487">Paper</a> / <a href="https://youtu.be/FhUNtxLlDkU" > Video </a>
              <p> In this work, we introduce <strong>Optim-FKD</strong>, an approach for high-speed navigation that uses a learned forward kinodynamics model (FKD) coupled with non-linear least squares optimization for multi-horizon control. </p>
            </td>
        </tr>

        <!--          BARN Challenge -->
        <tr>
          <td style="padding:25px;width:25%;vertical-align:middle">
            <img src="images/barn_at_icra.jpg" alt="3DSP" width="160" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2208.10473" id="3DSP">
              <papertitle>Autonomous Ground Navigation in Highly Constrained Spaces: Lessons Learned From the Benchmark Autonomous Robot Navigation Challenge at ICRA 2022</papertitle>
            </a>
            Xuesu Xiao, Zifan Xu, Zizhao Wang, Yunlong Song, Garrett Warnell, Peter Stone, Tingnan Zhang, Shravan Ravi, Gary Wang, <strong>Haresh Karnan</strong>, Joydeep Biswas, Nicholas Mohammad, Lauren Bramblett, Rahul Peddi, Nicola Bezzo, Zhanteng Xie, Philip Dames
            <br>
            <em>Robotics and Automation Magazine (RA-M) </em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2208.10473">Paper</a>
            <p> Won the 1st place in BARN Challenge @ ICRA 2022 </p>
          </td>
        </tr>

<!--          VOILA -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/voila_airsim.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2105.09371" id="3DSP">
                <papertitle>VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2105.09371">Paper</a> / <a href="https://www.youtube.com/watch?v=aFspSnjnw-k" > Video </a> / <a href="data/VOILA_poster.pdf"> Poster </a>
              <p> Imitating an expert's video-only demonstration can be challenging in the presence of egocentric viewpoint mismatch. In <strong> VOILA</strong>, we introduce a unique viewpoint-invariant reward function to effectively imitate demonstrations on a physically different agent. </p>
            </td>
          </tr>

<!--          VGAIfO-SO -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle">
              <img src="images/vgaifo-so.png" alt="3DSP" width="150" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2202.00243" id="3DSP">
                <papertitle>Adversarial Imitation Learning from Video using a State Observer</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/users/faraztrb/">Faraz Torabi</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2202.00243">Paper</a> / <a href="https://www.youtube.com/watch?v=q21OCKJPXNo&ab_channel=Hareshkarnan"> Video </a> / <a href="data/VGAIfO-SO_poster.pdf"> Poster </a>
              <p> SOTA approaches in Imitation from Video-only demonstrations exhibit poor sample efficiency when learning with access to proprioceptive features of the imitator. To tackle this, we introduce <strong>VGAIfO-SO</strong>, an IfO algorithm that improves sample efficiency through self-supervision. </p>
            </td>
          </tr>

<!--          Grounded Sim-to-real article-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gsim2real.png" alt="3DSP" width="135" height="145" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s10994-021-05982-z" id="3DSP">
                <papertitle>Grounded Action Transformation for Sim-to-Real Reinforcement Learning</papertitle>
              </a>
              <br>
               <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Siddharth Desai, <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Springer. Machine Learning</em>, 2021
              <br>
              <a href="https://link.springer.com/article/10.1007/s10994-021-05982-z">Paper</a>
              <p> Sim-to-real is the problem of learning a control policy in an inaccurate simulated world such that the learned policy when transferred to the real-world, performs well. In this article, we explore the proposed black-box Sim-to-Real algorithm GAT, and its extension SGAT. </p>
            </td>
          </tr>

<!--          GARAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/minitaur.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/NEURIPS20-Karnan.pdf" id="3DSP">
                <papertitle>An Imitation from Observation Approach to Transfer Learning with Dynamics Mismatch</papertitle>
              </a>
              <br>
               Siddharth Desai, <a href="https://idurugkar.github.io/">Ishan Durugkar</a>, <strong>Haresh Karnan</strong>, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br>
              <a href="https://neurips.cc/virtual/2020/protected/poster_28f248e9279ac845995c4e9f8af35c2b.html">NeurIPS site</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/NEURIPS20-Karnan.pdf">Paper</a> / <a href="data/GARAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01594">arXiv</a>
              <p> In this work, we propose the <strong>GARAT</strong> algorithm, which treats the Sim-to-Real problem as an Imitation from Observation (IfO) problem and uses advances in the IfO literature to transfer a control policy from a source domain to a target domain, using Adversarial Imitation Learning.</p>
            </td>
          </tr>

<!--          SGAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sgat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Desai.pdf" id="3DSP">
                <papertitle>Stochastic Grounded Action Transformation for Robot Learning in Simulation </papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Siddharth Desai, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="https://www.youtube.com/watch?v=rz1s7uft-ow&feature=youtu.be&ab_channel=SiddharthDesai">Long Video</a> / <a href="https://youtu.be/StNQIq3o-Qw">Short Video</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Desai.pdf">Paper</a> / <a href="data/SGAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01281">arXiv</a>
              <p> Real world dynamics are often stochastic and robot simulators have an inaccurate approximation of real world dynamics. In this work, we propose a Sim-to-Real algorithm called <strong>SGAT</strong> and transfer a Humanoid walk from Simulation to Real world.</p>
            </td>
          </tr>

<!--          RGAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rgat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
                <papertitle>Reinforced Grounded Action Transformation for Sim-to-Real Transfer</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Siddharth Desai, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="https://www.youtube.com/watch?v=mInoJkzBP9M&feature=youtu.be&ab_channel=HareshKarnan">Long Video</a> / <a href="https://youtu.be/Hfu_d44IGAw">Short Video</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf">Paper</a> / <a href="data/RGAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01279">arXiv</a>
              <p> In this work, we propose a Sim-to-Real algorithm called <strong>RGAT</strong> to ground an inaccurate simulator with data from the real world, using Reinforcement Learning.</p>
            </td>
          </tr>

<!--          Tensegrity IROS 2017-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros2017.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/document/8206022" id="3DSP">
                <papertitle>Visual Feedback Control of Tensegrity Robotic Systems</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Raman Goyal, Manoranjan Majji, Robert Skelton, Puneet Singla
              <br>
              <em>International Conference on Intelligent Robots and Systems</em>, 2017
              <br>
              <a href="https://www.youtube.com/watch?v=8uLkJeiPAeI&ab_channel=HareshKarnan">Video</a> / <a href="https://ieeexplore.ieee.org/document/8206022">Paper</a>
              <p> Tensegrity mechanisms are known for their minimal-mass and flexible properties. In this work, we propose using vision based sensing for shape control of such soft robotic manipulators. </p>
            </td>
          </tr>

        <!--  Others-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Workshops, Symposiums, Extended Abstracts</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!-- IPPC Workshop poster -->
        <tr>
          <td style="padding:25px;width:25%;vertical-align:middle">
            <img src="images/ippc_poster.PNG" alt="3DSP" width="160" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://ippc-iros23.github.io/papers/karnan.pdf" id="3DSP">
              <papertitle>Aligning Robot Navigation Behaviors with Human Intentions and Preferences</papertitle>
            </a>
            <br>
              <strong>Haresh Karnan</strong>
            <br>
            <em>IPPC: Integrated Planning, Perception and Control Workshop, IROS</em>, 2023
            <br>
            <a href="https://ippc-iros23.github.io/papers/karnan.pdf">Paper</a> / <a href="https://drive.google.com/file/d/1EoD-AMSxJD_8KKyJ3-vBHoV7nTafUGjB/view?usp=sharing"> Poster </a>
            <p> The research explored in this paper helps address challenges of value alignment in machine learning for robot navigation. It presents new algorithms and a dataset to teach robots navigation behaviors that align with human intentions and preferences. These innovations are applicable in diverse settings, including indoor, outdoor, and human-occupied environments.</p>
          </td>
        </tr>
        
        <!-- Principles and Guidelines for Evaluating Social Robot Navigation Algorithms -->
        <tr>
          <td style="padding:25px;width:25%;vertical-align:middle">
            <img src="images/socnav.jpg" alt="3DSP" width="160" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2306.16740" id="3DSP">
              <papertitle>Principles and Guidelines for Evaluating Social Robot Navigation Algorithms</papertitle>
            </a>
            <br>
            Anthony Francis, Claudia Perez-D'Arpino, Chengshu Li, Fei Xia,..., <strong>Haresh Karnan</strong>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
            <br>
            <em>ArXiv</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2306.16740">Paper</a>
            <p> We introduce a standardized criteria and a metrics framework for evaluating social robot navigation in human occupied spaces. Key principles include safety, comfort, and social competency. This approach fosters fair algorithm comparison across diverse robots, simulators, and datasets, accelerating progress akin to computer vision and traditional robot navigation.</p>
          </td>
        </tr>

      <!--  Social Nav Benchmark-->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/socialnavdallle.png" alt="3DSP" width="160" height="120" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
            <papertitle>Benchmarking Social Robot Navigation Across Academia and Industry</papertitle>
          </a>
          <br>
          Anthony Francis, Claudia Perez-D'Arpino, Chengshu Li, Fei Xia, Alexandre Alahi, Aniket Bera, Abhijat Biswas, Joydeep Biswas, Hao-Tien Lewis Chiang, Michael Everett, Sehoon Ha, Justin Hart, <strong>Haresh Karnan</strong>, Tsang-Wei Edward Lee, Luis J. Manso, Reuth Mirksy, Soren Pirk, Phani Teja Singamaneni, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>, Ada V. Taylor, Peter Trautman, Nathan Tsoi, Marynel Vazquez, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Peng Xu, Naoki Yokoyama, Roberto Martƒ±n-Martƒ±n, Alexander Toshev
          <br>
          <em>HRI in Academia and Industry, AAAI Spring Symposum, 2023 <strong>(Best Paper Award Nominee)</strong></em>
          <br>
          <a href="https://drive.google.com/file/d/1WwlIEIW0fTpxy8il8Mn87KIaAXloibs1/view">Paper</a>
        </td>
      </tr>

      <!--  VOILA-->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/voila_airsim.gif" alt="3DSP" width="160" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
              <papertitle>VOILA: Visual Observation-only Imitation Learning for Autonomous navigation</papertitle>
            </a>
            <br>
            <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
            <br>
            <em>AAAI ML4NAV Spring Symposium</em>, 2021
            <br>
            <a href="https://drive.google.com/file/d/1R1hFWiPKTn7jhb1a1T5lbwVTcjEdK5Z3/view">Talk</a> / <a href="https://drive.google.com/file/d/135FCPsHu8sg72zzGhv6CJg3AHYajDF5O/view">Paper</a>
          </td>
        </tr>

      <!--  take out the trash-->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/taketrash.gif" alt="3DSP" width="160" height="120" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/pdf/1909.06529.pdf" id="3DSP">
              <papertitle>Solving Service Robot Tasks: UT Austin Villa@Home 2019 Team Report</papertitle>
            </a>
            <br>
            Rishi Shah, Yuqian Jiang, <strong>Haresh Karnan</strong>, Gilberto Briscoe-Martinez, Dominick Mulder, Ryan Gupta, Rachel Schlossman, Marika Murphy, Justin W. Hart, Luis Sentis, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
            <br>
            <em>AAAI AI-HRI Symposium</em>, 2019
            <br>
            <a href="https://www.youtube.com/watch?v=Z8G2p7bkx3k&feature=youtu.be&ab_channel=RishiShah">Video</a> / <a href="https://arxiv.org/pdf/1909.06529.pdf">Paper</a> / <a href="https://arxiv.org/abs/1909.06529">arXiv</a>
          </td>
        </tr>

    <!--          GARAT - RSS-->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/garat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://sim2real.github.io/assets/papers/2020/desai.pdf" id="3DSP">
            <papertitle>Extended Abstract: An Imitation from Observation Approach to Sim-to-Real Transfer</papertitle>
          </a>
          <br>
           Siddharth Desai, <a href="https://idurugkar.github.io/">Ishan Durugkar</a>, <strong>Haresh Karnan</strong>, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
          <br>
          <em>Robotics Science and Systems, Sim-to-Real Workshop (RSS)</em>, 2020
          <br>
          <a href="https://www.youtube.com/watch?v=CEEGJZJmDF8&feature=youtu.be&ab_channel=HareshKarnan">Video</a> / <a href="https://sim2real.github.io/assets/papers/2020/desai.pdf">Paper</a> / <a href="data/GARAT_poster.pdf">Poster</a>
          <p> In this work, we propose the <strong>GARAT</strong> algorithm, which treats the Sim-to-Real problem as an Imitation from Observation (IfO) problem and uses advances in the IfO literature to transfer a control policy from a source domain to a target domain, using Adversarial Imitation Learning.</p>
        </td>
      </tr>


    <!--          USV tracking-->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/trackusv.gif" alt="3DSP" width="160" height="120" style="border-style: none">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/1710.02932.pdf" id="3DSP">
            <papertitle>Visual Servoing of Unmanned Surface Vehicle from Small Tethered Unmanned Aerial Vehicle</papertitle>
          </a>
          <br>
          <strong>Haresh Karnan</strong>, Aritra Biswas, Pranav Vaidik Dhulipala, Jan Dufek, Robin Murphy
          <br>
          <em>arXiv preprint</em>, 2017
          <br>
          <a href="https://arxiv.org/pdf/1710.02932.pdf">Paper</a> / <a href="https://arxiv.org/abs/1710.02932">arXiv</a>
        </td>
      </tr>
    
    <!-- Patents -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Patents</heading>
      </td>
    </tr>
  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/patent.jpeg" alt="3DSP" width="160" height="120" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/1710.02932.pdf" id="3DSP">
          <papertitle>Systems and Methods for Utilizing Images to Determine the Position and Orientation of a Vehicle</papertitle>
        </a>
        <!-- <br> -->
        Alan Atherton, Umit Batur, Kishor Bhalerao, <strong>Haresh Karnan</strong>, Harshavardhan Shirolkar
        <br>
        <em>USPTO, US11579622B2</em>, 2021
        <br>
        <a href="https://patents.google.com/patent/US11579622B2/en">Google Patents</a> / <a href="https://www.autoevolution.com/news/new-tech-could-help-self-driving-cars-figure-out-where-they-are-even-without-gps-167185.html#:~:text=A%20patent%20called%20%E2%80%9C%20Systems%20and%20methods%20for,shortcoming%3A%20the%20reliance%20on%20GPS%20for%20autonomous%20vehicles.">News Coverage</a>
      </td>
    </tr>

<!--        <table width="100%" align="center" border="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/cs188.jpg" alt="cs188">-->
<!--            </td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

<!--        credits-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info/">Template Credits</a>
                </font>
              </p>
            </td>
          </tr>
        </tbody>
      </td>
    </tr>
  </table>
</body>

</html>
