<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haresh Karnan</title>

  <meta name="author" content="Haresh Karnan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/ut_seal.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haresh Karnan</name>
              </p>
              <p>I am a fourth year PhD student at The University of Texas at Austin, working on Reinforcement Learning, Computer Vision and Artificial Intelligence for Robotics. I'm a member of the Learning Agents Research Group (LARG), and advised by <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a> in the Computer Science department. I'm also a part of the <a href="https://www.cs.utexas.edu/~AustinVilla/?p=athome20">UT Austin RoboCup@Home</a> team.
              </p>

<!--              I'm currently focused on learning to imitation navigation skills from raw video observations.-->

              <p>
                I've previously interned at <a href="https://www.aboutamazon.com/news/transportation/meet-scout">Amazon Scout</a> where I worked on Vision based Robot Localization for their package delivery robot - <strong>Scout</strong>.
              </p>

              <p>
                In my past life, I worked with <a href="https://bobskelton.github.io/">Dr. Robert Skelton</a> at Texas A&M University, College Station, on localization, sensing and control of Tensegrity robots.
              </p>

              <p style="text-align:center">
                <a href="mailto:haresh.miriyala@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="data/Haresh_Karnan_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="data/HareshKarnan-bio.txt">Biography</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VatfufAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/hareshmiriyala">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hareshkarnan/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/haresh_new.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/haresh_new_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Pre-Prints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:25px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
              <img src="images/scand.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2105.09371" id="3DSP">
                <papertitle>Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation</papertitle>
              </a>
               <strong>Haresh Karnan</strong>, Anirudh Nair, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Garrett Warnell, <a href="https://storage.googleapis.com/pirk.io/index.html">Soeren Pirk</a>, <a href="https://sites.google.com/view/alextoshev">Alexander Toshev</a>, Justin Hart, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <a href="https://arxiv.org/abs/2203.15041">Paper</a> / <a href="https://www.youtube.com/watch?v=QqfgWmjOKkY" > Video </a>
              <p> We introduce a Large-Scale, first-person-view dataset of socially compliant robot navigation demonstrations. <strong>SCAND</strong> consists of 138 trajectories, 25 miles of socially compliant navigation demonstrations collected on 2 robots by 4 human demonstrators within the UT Austin campus. </p>
            </td>
          </tr>

        <tr>
            <td style="padding:25px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
              <img src="images/viikd.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2105.09371" id="3DSP">
                <papertitle>VI-IKD: High-Speed Accurate Off-Road Navigation using Learned Visual-Inertial Inverse Kinodynamics</papertitle>
              </a>
               <strong>Haresh Karnan</strong>, Kavan Singh Sikand, Pranav Atreya, <a href="https://www.cs.utexas.edu/~srabiee/">Sadegh Rabiee</a>, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, Garrett Warnell, <a href="https://www.joydeepb.com/">Joydeep Biswas</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <a href="https://arxiv.org/abs/2203.15983">Paper</a> / <a href="https://youtu.be/WZDLclg5BPk" > Video </a>
              <p> In this work, we hypothesize that to enable high-speed off-road navigation, in addition to incorporating inertial information, one must also anticipate the kinodynamic interactions of the vehicle with the terrain in the future. Our <strong>VI-IKD</strong> algorithm learns an IKD model that is conditioned on inertial and visual information of a patch of terrain ahead. </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!--          VOILA -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
              <img src="images/voila_airsim.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2105.09371" id="3DSP">
                <papertitle>VOILA: Visual-Observation-Only Imitation Learning for Autonomous Navigation</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2105.09371">Paper</a> / <a href="https://www.youtube.com/watch?v=aFspSnjnw-k" > Video </a> / <a href="data/VOILA_poster.pdf"> Poster </a>
              <p> Learning to imitate from an expert's video-only demonstration can be hard when there is significant viewpoint mismatch between the expert and the imitator agents. In this work, we propose <strong>VOILA</strong>, an IfO algorithm that imitates an expert driver's navigation policy from a single video-only demonstration, overcoming viewpoint mismatch. </p>
            </td>
          </tr>

<!--          VGAIfO-SO -->
          <tr>
            <td style="padding:25px;width:25%;vertical-align:middle" bgcolor="#ffffd0">
              <img src="images/vgaifo-so.png" alt="3DSP" width="150" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2202.00243" id="3DSP">
                <papertitle>Adversarial Imitation Learning from Video using a State Observer</papertitle>
              </a>
              <br>
               <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/users/faraztrb/">Faraz Torabi</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2202.00243">Paper</a> / <a href="https://www.youtube.com/watch?v=q21OCKJPXNo&ab_channel=Hareshkarnan"> Video </a>
              <p> SOTA approaches in Imitation from Video only demonstrations exhibit poor sample efficiency when learning with access to proprioceptive features of the imitator. To tackle this, we introduce <strong>VGAIfO-SO</strong>, an IfO algorithm that uses self-supervision to improve sample efficiency. </p>
            </td>
          </tr>

<!--          Grounded Sim-to-real article-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gsim2real.png" alt="3DSP" width="135" height="145" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/article/10.1007/s10994-021-05982-z" id="3DSP">
                <papertitle>Grounded Action Transformation for Sim-to-Real Reinforcement Learning</papertitle>
              </a>
              <br>
               <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Siddharth Desai, <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Springer. Machine Learning</em>, 2021
              <br>
              <a href="https://link.springer.com/article/10.1007/s10994-021-05982-z">Paper</a>
              <p> Sim-to-real is the problem of learning a control policy in an inaccurate simulated world such that the learned policy when transferred to the real-world, performs well. In this article, we explore the proposed black-box Sim-to-Real algorithm GAT, and its extension SGAT. </p>
            </td>
          </tr>

<!--          GARAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/minitaur.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/NEURIPS20-Karnan.pdf" id="3DSP">
                <papertitle>An Imitation from Observation Approach to Transfer Learning with Dynamics Mismatch</papertitle>
              </a>
              <br>
               Siddharth Desai, <a href="https://idurugkar.github.io/">Ishan Durugkar</a>, <strong>Haresh Karnan</strong>, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br>
              <a href="https://neurips.cc/virtual/2020/protected/poster_28f248e9279ac845995c4e9f8af35c2b.html">NeurIPS site</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/NEURIPS20-Karnan.pdf">Paper</a> / <a href="data/GARAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01594">arXiv</a>
              <p> In this work, we propose the <strong>GARAT</strong> algorithm, which treats the Sim-to-Real problem as an Imitation from Observation (IfO) problem and uses advances in the IfO literature to transfer a control policy from a source domain to a target domain, using Adversarial Imitation Learning.</p>
            </td>
          </tr>

<!--          SGAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sgat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Desai.pdf" id="3DSP">
                <papertitle>Stochastic Grounded Action Transformation for Robot Learning in Simulation </papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Siddharth Desai, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="https://www.youtube.com/watch?v=rz1s7uft-ow&feature=youtu.be&ab_channel=SiddharthDesai">Long Video</a> / <a href="https://youtu.be/StNQIq3o-Qw">Short Video</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Desai.pdf">Paper</a> / <a href="data/SGAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01281">arXiv</a>
              <p> Real world dynamics are often stochastic and robot simulators have an inaccurate approximation of real world dynamics. In this work, we propose a Sim-to-Real algorithm called <strong>SGAT</strong> and transfer a Humanoid walk from Simulation to Real world.</p>
            </td>
          </tr>

<!--          RGAT-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rgat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
                <papertitle>Reinforced Grounded Action Transformation for Sim-to-Real Transfer</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Siddharth Desai, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2020
              <br>
              <a href="https://www.youtube.com/watch?v=mInoJkzBP9M&feature=youtu.be&ab_channel=HareshKarnan">Long Video</a> / <a href="https://youtu.be/Hfu_d44IGAw">Short Video</a> / <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf">Paper</a> / <a href="data/RGAT_poster.pdf">Poster</a> / <a href="https://arxiv.org/abs/2008.01279">arXiv</a>
              <p> In this work, we propose a Sim-to-Real algorithm called <strong>RGAT</strong> to ground an inaccurate simulator with data from the real world, using Reinforcement Learning.</p>
            </td>
          </tr>

<!--          Tensegrity IROS 2017-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros2017.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                <papertitle>Visual Feedback Control of Tensegrity Robotic Systems</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Raman Goyal, Manoranjan Majji, Robert Skelton, Puneet Singla
              <br>
              <em>International Conference on Intelligent Robots and Systems</em>, 2017
              <br>
              <a href="https://www.youtube.com/watch?v=8uLkJeiPAeI&ab_channel=HareshKarnan">Video</a> / <a href="https://www.researchgate.net/profile/Raman_Goyal4/publication/321821228_Visual_feedback_control_of_tensegrity_robotic_systems/links/5a9d735aaca2721e3f33d72d/Visual-feedback-control-of-tensegrity-robotic-systems.pdf">Paper</a> / <a href="https://ieeexplore.ieee.org/document/8206022">IEEE Xplore</a>
              <p> Tensegrity mechanisms are known for their minimal-mass and flexible properties. In this work, we propose using vision based sensing for shape control of such soft robotic manipulators. </p>
            </td>
          </tr>

        <!--  Others-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Workshops, Symposiums, Extended Abstracts</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <!--  VOILA-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/voila_airsim.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
                <papertitle>VOILA: Visual Observation-only Imitation Learning for Autonomous navigation</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~xiao/">Xuesu Xiao</a>, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>AAAI ML4NAV Spring Symposium</em>, 2021
              <br>
              <a href="https://drive.google.com/file/d/1R1hFWiPKTn7jhb1a1T5lbwVTcjEdK5Z3/view">Talk</a> / <a href="https://drive.google.com/file/d/135FCPsHu8sg72zzGhv6CJg3AHYajDF5O/view">Paper</a>
            </td>
          </tr>

        <!--  take out the trash-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/taketrash.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IROS20-Karnan.pdf" id="3DSP">
                <papertitle>Solving Service Robot Tasks: UT Austin Villa@Home 2019 Team Report</papertitle>
              </a>
              <br>
              Rishi Shah, Yuqian Jiang, <strong>Haresh Karnan</strong>, Gilberto Briscoe-Martinez, Dominick Mulder, Ryan Gupta, Rachel Schlossman, Marika Murphy, Justin W. Hart, Luis Sentis, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>AAAI AI-HRI Symposium</em>, 2019
              <br>
              <a href="https://www.youtube.com/watch?v=Z8G2p7bkx3k&feature=youtu.be&ab_channel=RishiShah">Video</a> / <a href="https://arxiv.org/pdf/1909.06529.pdf">Paper</a> / <a href="https://arxiv.org/abs/1909.06529">arXiv</a>
            </td>
          </tr>

<!--          GARAT - RSS-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/garat.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://sim2real.github.io/assets/papers/2020/desai.pdf" id="3DSP">
                <papertitle>Extended Abstract: An Imitation from Observation Approach to Sim-to-Real Transfer</papertitle>
              </a>
              <br>
               Siddharth Desai, <a href="https://idurugkar.github.io/">Ishan Durugkar</a>, <strong>Haresh Karnan</strong>, <a href="https://www.cs.utexas.edu/~jphanna/">Josiah Hanna</a>, Garrett Warnell, <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>
              <br>
              <em>Robotics Science and Systems, Sim-to-Real Workshop (RSS)</em>, 2020
              <br>
              <a href="https://www.youtube.com/watch?v=CEEGJZJmDF8&feature=youtu.be&ab_channel=HareshKarnan">Video</a> / <a href="https://sim2real.github.io/assets/papers/2020/desai.pdf">Paper</a> / <a href="data/GARAT_poster.pdf">Poster</a>
              <p> In this work, we propose the <strong>GARAT</strong> algorithm, which treats the Sim-to-Real problem as an Imitation from Observation (IfO) problem and uses advances in the IfO literature to transfer a control policy from a source domain to a target domain, using Adversarial Imitation Learning.</p>
            </td>
          </tr>


        <!--          USV tracking-->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/trackusv.gif" alt="3DSP" width="160" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmbG1tOGIta3N1Wjg/view?usp=sharing" id="3DSP">
                <papertitle>Visual Servoing of Unmanned Surface Vehicle from Small Tethered Unmanned Aerial Vehicle</papertitle>
              </a>
              <br>
              <strong>Haresh Karnan</strong>, Aritra Biswas, Pranav Vaidik Dhulipala, Jan Dufek, Robin Murphy
              <br>
              <em>arXiv preprint</em>, 2017
              <br>
              <a href="https://arxiv.org/pdf/1710.02932.pdf">Paper</a> / <a href="https://arxiv.org/abs/1710.02932">arXiv</a>
            </td>
          </tr>


<!--        <table width="100%" align="center" border="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/cs188.jpg" alt="cs188">-->
<!--            </td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

<!--        credits-->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <a href="https://jonbarron.info/">Template Credits</a>
                </font>
              </p>
            </td>
          </tr>
        </tbody>
      </td>
    </tr>
  </table>
</body>

</html>
